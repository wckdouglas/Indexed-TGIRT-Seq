
#!/bin/env python

from functools import partial
from Bio.SeqIO.QualityIO import FastqGeneralIterator
from sys import stderr
import numpy as np
import sys
import argparse
import glob
import gzip
import time
import os
from itertools import izip
from multiprocessing import Pool
from collections import defaultdict
import pyximport
pyximport.install(setup_args={'include_dirs': np.get_include()})
from cluster_reads import (dictToJson,
                           writingAndClusteringReads,
                           plotBCdistribution,
                           hammingDistance)

programname = os.path.basename(sys.argv[0]).split('.')[0]

def getOptions():
    '''
    reading input
    '''
    descriptions = 'Clustering fastq reads to fasta reads with the first $idx_base bases as cDNA-synthesis barcode. ' +\
                'Concensus bases are called only when the fraction of reads that contain the concensus base exceed some threshold. '+ \
                'Quality scores are generated by the average score for the bases that matched concensus base. '
    parser = argparse.ArgumentParser(description=descriptions)
    parser.add_argument('-o', '--outputprefix', required=True,
        help='Paired end Fastq files with R1_001.fastq.gz as suffix for read1, and R2_001.fastq.gz as suffix for read2')
    parser.add_argument('-1', '--fastq1', required=True,
        help='Paired end Fastq file 1 with four line/record')
    parser.add_argument('-2', '--fastq2',required=True,
        help='Paired end Fastq file 2 with four line/record')
    parser.add_argument('-m', '--cutoff', type=int,default=4,
        help="minimum read count for each read cluster (default: 4)")
    parser.add_argument("-x", "--idxBase", type=int, default=13,
        help="how many base in 5' end as index? (default: 13)")
    parser.add_argument('-q', '--barcodeCutOff', type=int, default=30,
        help="Average base calling quality for barcode sequence (default=30)")
    parser.add_argument("-c", "--constant_region", default='CATCG',
        help="Constant sequence after tags (default: CATCG ,e.g. Douglas's index-R1R)")
    parser.add_argument("-t", "--threads", type=int,default=1,
        help="Threads to use (deflaut: 1)")
    parser.add_argument("-a", "--mismatch", type=int,default=1,
        help="Allow how many mismatch in constant region (deflaut: 1)")
    args = parser.parse_args()
    return args


def readClustering(barcode_dict, idx_base, barcode_cut_off, constant, constant_length, hamming_threshold, usable_seq, read1, read2):
    """
    generate read cluster with a dictionary object and seqRecord class.
    index of the dictionary is the barcode extracted from first /idx_bases/ of read 1
    """
    idLeft, seqLeft, qualLeft = read1
    idRight, seqRight, qualRight = read2
    assert idLeft.split(' ')[0] == idRight.split(' ')[0], 'Wrongly splitted files!! %s\n%s' %(idRight, idLeft)
    barcode = seqLeft[:idx_base]
    constant_region = seqLeft[idx_base:usable_seq]
    barcodeQualmean = int(np.mean(map(ord,qualLeft[:idx_base])) - 33)

    no_N_barcode = 'N' not in barcode
    low_complexity_barcode = any(pattern in barcode for pattern in ['AAAAA','CCCCC','TTTTT','GGGGG'])
    hiQ_barcode = barcodeQualmean > barcode_cut_off
    accurate_constant = hammingDistance(constant, constant_region) <= hamming_threshold

    if no_N_barcode and hiQ_barcode and accurate_constant:
            #and not low_complexity_barcode:
        seqLeft = seqLeft[usable_seq:]
        qualLeft = qualLeft[usable_seq:]
        barcode_dict[barcode].append([seqLeft,seqRight,qualLeft, qualRight])
        return 0
    return 1



def recordsToDict(outputprefix, inFastq1, inFastq2, idx_base, barcode_cut_off,
                constant, barcode_dict, allow_mismatch):
    discarded_sequence_count = 0
    constant_length = len(constant)
    hamming_threshold = float(allow_mismatch)/constant_length
    usable_seq = idx_base + constant_length

    cluster_reads = partial(readClustering, barcode_dict, idx_base, barcode_cut_off,
                            constant, constant_length, hamming_threshold, usable_seq)
    with gzip.open(inFastq1,'rb') as fq1, gzip.open(inFastq2,'rb') as fq2:
        iterator = enumerate(izip(FastqGeneralIterator(fq1),FastqGeneralIterator(fq2)))
        for read_num, (read1,read2) in iterator:
            discarded_sequence_count += cluster_reads(read1, read2)
            if read_num % 1000000 == 0:
                stderr.write('[%s] Parsed: %i sequence\n' %(programname,read_num))

    barcode_count = len(barcode_dict.keys())
    stderr.write('[%s] Extracted: %i barcode group\n' %(programname,barcode_count) +\
                 '[%s] discarded: %i sequences\n' %(programname, discarded_sequence_count) +\
                 '[%s] Parsed:    %i seqeucnes\n' %(programname, read_num))
    return barcode_dict, read_num, barcode_count


def clustering(outputprefix, inFastq1, inFastq2, idx_base, min_family_member_count,
               barcode_cut_off, constant, threads, allow_mismatch):
    json_file = outputprefix+'.json'
    barcode_dict = defaultdict(list)
    barcode_dict, read_num, barcode_count = recordsToDict(outputprefix, inFastq1, inFastq2, idx_base,
                                                        barcode_cut_off, constant, barcode_dict)
    barcode_member_counts = map(lambda index: len(barcode_dict[index]), barcode_dict.keys())
    p = plotBCdistribution(barcode_member_counts, outputprefix)
    dictToJson(barcode_dict, json_file)
    barcode_dict.clear()
    output_cluster_count, read1File, read2File = writingAndClusteringReads(outputprefix, min_family_member_count, json_file, threads)
    # all done!
    stderr.write('[%s] Finished writing error free reads\n' %programname)
    stderr.write('[%s] [Summary]                        \n' %programname)
    stderr.write('[%s] read1:                     %s\n' %(programname, read1File))
    stderr.write('[%s] read2:                     %s\n' %(programname, read2File))
    stderr.write('[%s] output clusters:           %i\n' %(programname, output_cluster_count))
    stderr.write('[%s] Percentage retained:       %.3f\n' %(programname, float(output_cluster_count)/read_num * 100))
    return 0

def main(args):
    """
    main function:
        controlling work flow
        1. generate read clusters by reading from fq1 and fq2
        2. obtain concensus sequence from read clusters
        3. writing concensus sequence to files
    """
    start = time.time()
    outputprefix = args.outputprefix
    inFastq1 = args.fastq1
    inFastq2 = args.fastq2
    idx_base = args.idxBase
    min_family_member_count = args.cutoff
    barcode_cut_off = args.barcodeCutOff
    constant = args.constant_region
    threads = args.threads
    allow_mismatch = args.mismatch

    #print out parameters
    stderr.write('[%s] [Parameters] \n' %(programname))
    stderr.write('[%s] indexed bases:                     %i\n' %(programname,idx_base))
    stderr.write('[%s] minimum coverage:                  %i\n' %(programname,min_family_member_count))
    stderr.write('[%s] outputPrefix:                      %s\n' %(programname,outputprefix))
    stderr.write('[%s] threads:                           %i\n' %(programname,threads))
    stderr.write('[%s] using constant regions:            %s\n' %(programname,constant))
    stderr.write('[%s] allowed mismatches:                %i\n' %(programname, allow_mismatch))

    # divide reads into subclusters
    clustering(outputprefix, inFastq1, inFastq2, idx_base, min_family_member_count,
               barcode_cut_off, constant, threads, allow_mismatch)

    stderr.write('[%s] time lapsed:      %2.3f min\n' %(programname, np.true_divide(time.time()-start,60)))
    return 0

if __name__ == '__main__':
    args = getOptions()
    main(args)
