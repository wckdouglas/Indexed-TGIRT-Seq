#!/bin/env python

import matplotlib
matplotlib.use('Agg')
from Bio.SeqIO.QualityIO import FastqGeneralIterator
from sys import stderr
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sys
import argparse
import glob
import gzip
import time
import os
from itertools import izip, product, imap
from multiprocessing import Pool, Manager
import progressbar
from cluster_reads import *
programname = os.path.basename(sys.argv[0]).split('.')[0]

#======================  starting functions =============================
def getOptions():
    '''reading input 
    '''
    descriptions = 'Clustering fastq reads to fasta reads with the first $IDXBASE bases as cDNA-synthesis barcode. ' +\
                'Concensus bases are called only when the fraction of reads that contain the concensus base exceed some threshold. '+ \
                'Quality scores are generated by the average score for the bases that matched concensus base. ' 
    parser = argparse.ArgumentParser(description=descriptions)
    parser.add_argument('-o', '--outputprefix', required=True,
        help='Paired end Fastq files with R1_001.fastq.gz as suffix for read1, and R2_001.fastq.gz as suffix for read2')
    parser.add_argument('-1', '--fastq1', required=True,
        help='Paired end Fastq file 1 with four line/record')
    parser.add_argument('-2', '--fastq2',required=True,
        help='Paired end Fastq file 2 with four line/record')
    parser.add_argument('-m', '--cutoff', type=int,default=4,
        help="minimum read count for each read cluster (default: 4)")
    parser.add_argument("-x", "--idxBase", type=int, default=13,
        help="how many base in 5' end as index? (default: 13)")
    parser.add_argument('-q', '--barcodeCutOff', type=int, default=30,
        help="Average base calling quality for barcode sequence (default=30)")
    parser.add_argument("-t", "--threads", type=int, default = 1,
        help="Threads to use (default: 1)")
    parser.add_argument("-c", "--constant_region", default='',
            help="Constant sequence after tags (default: '')")
    args = parser.parse_args()
    outputprefix = args.outputprefix
    inFastq1 = args.fastq1
    inFastq2 = args.fastq2
    idxBase = args.idxBase
    minReadCount = args.cutoff
    barcodeCutOff = args.barcodeCutOff
    threads = args.threads
    constant = args.constant_region
    return outputprefix, inFastq1, inFastq2, idxBase, minReadCount, barcodeCutOff, threads, constant

def concensusPairs(reads):
    """ given a pair of reads as defined as the class: seqRecord
    return concensus sequence and mean quality of the pairs, 
        as well as the number of reads that supports the concnesus pairs
    see function: concensusSeq, calculateConcensusBase
    """
    # get concensus left reads first
    sequenceLeft, qualityLeft = concensusSeq(reads.seqListLeft, reads.qualListLeft, range(np.unique(reads.readLengthLeft())[0]))
    assert len(sequenceLeft) == len(qualityLeft), 'Wrong concensus sequence and quality!'
    # get concensus right reads first
    sequenceRight, qualityRight = concensusSeq(reads.seqListRight, reads.qualListRight, range(np.unique(reads.readLengthRight())[0]))
    assert len(sequenceRight) == len(qualityRight), 'Wrong concensus sequence and quality!'
    return sequenceLeft, qualityLeft, len(reads.seqListLeft), sequenceRight, qualityRight, len(reads.seqListRight)

def selectSeqLength(readLengthArray):
    """
    Given a list of sequence length of a read cluster from either side of the pair,
    select the sequence length with highest vote
    """
    seqlength, count = np.unique(readLengthArray, return_counts=True)
    return seqlength[count==max(count)][0]

def errorFreeReads(readCluster, index, counter, minReadCount):
    """
    main function for getting concensus sequences from read clusters.
    return  a pair of concensus reads with a 4-line fastq format
    see functions: 1. filterRead, 
                  2. concensusPairs,
                  3. calculateConcensusBase
    """
    #if readCluster.readCounts() > minReadCount:
    #    reads = filterRead(readCluster)
    # skip if not enough sequences to perform voting
    if readCluster is not None and readCluster.readCounts() > minReadCount:
        sequenceLeft, qualityLeft, supportedLeftReads, sequenceRight, qualityRight, supportedRightReads = concensusPairs(readCluster)
	counter.value += 1
        count = counter.value
        leftRecord = '@cluster_%i_%s %i readCluster\n%s\n+\n%s\n' \
                %(count, index, supportedLeftReads, sequenceLeft, qualityLeft)
        rightRecord = '@cluster_%i_%s %i readCluster\n%s\n+\n%s\n' \
                %(count, index, supportedRightReads, sequenceRight, qualityRight)
        if count % 100000 == 0:
            stderr.write('[%s] Processed %i read clusters.\n' %(programname, count))
        return(leftRecord,rightRecord)

def readClustering(read1, read2, barcodeDict, idxBase, barcodeCutOff, constant, lock):
    """
    generate read cluster with a dictionary object and seqRecord class.
    index of the dictionary is the barcode extracted from first /idxBases/ of read 1 
    """
    idLeft, seqLeft, qualLeft = read1
    idRight, seqRight, qualRight = read2
    assert idLeft.split(' ')[0] == idRight.split(' ')[0], 'Wrongly splitted files!! %s\n%s' %(idRight, idLeft)
    barcode = seqLeft[:idxBase]
    qualLeft = map(ord,qualLeft)
    qualRight = map(ord,qualRight)
    constant_length = len(constant)
    constant_region = seqLeft[idxBase:idxBase+constant_length] if constant_length > 0 else 0
    barcodeQualmean = int(np.mean(qualLeft[:idxBase]) - 33)
    if ('N' not in barcode \
	    and barcodeQualmean > barcodeCutOff \
	    and not any(pattern in barcode for pattern in ['AAAAA','CCCCC','TTTTT','GGGGG']) \
	    and hammingDistance(constant, constant_region) < 0.3) :
        seqLeft = seqLeft[idxBase+constant_length:]
	lock.acquire()
	record = barcodeDict.get(barcode,seqRecord()) 
	record.addRecord(seqRight, qualRight, seqLeft, qualLeft)
	barcodeDict[barcode] = record
	lock.release()
    return 0

def clustering(outputprefix, inFastq1, inFastq2, idxBase, minReadCount, barcodeCutOff, threads, constant):
    barcodeDict = Manager().dict({})
    lock = Manager().Lock()
    with open(inFastq1,'rb') as fq1, open(inFastq2,'rb') as fq2:
	pool = Pool(threads)
	for read1, read2 in izip(FastqGeneralIterator(fq1),FastqGeneralIterator(fq2)):
	    args = (read1, read2,barcodeDict, idxBase, barcodeCutOff, constant, lock)
	    pool.apply_async(readClustering,args = args).get()
	pool.close()
	pool.join()
    barcodeCount = [barcodeDict[key].readCounts() for key in barcodeDict.keys()]
    barcodeCount = np.array(barcodeCount, dtype=np.int64)

    # From index library, generate error free reads
    # using multicore to process read clusters
    counter = Manager().Value('i',0)
    pool = Pool(threads)
    iterable = barcodeDict.items()
    results = [pool.apply_async(errorFreeReads, args=(family, index, counter, minReadCount)).get() for index, family in iterable]
    pool.close()
    pool.join()
    results = filter(None,  results)
    # since some cluster that do not have sufficient reads
    # will return None, results need to be filtered
    if (len(results) == 0):
	return 0,0,0,len(barcodeDict)
    #    sys.exit('[%s] No concensus clusters!! \n' %(programname))
    left, right = zip(*results)
    return left, right, barcodeCount, len(barcodeDict)

def openTempFile(n,outputprefix):
    tempFiles = {}
    tempR1files = []
    tempR2files = []
    for splitCode in product('ACTG',repeat=n):
	prefix = ''.join(splitCode)
	splitR1File = outputprefix + '_' + prefix + '_R1_temp.fq'
	splitR2File = splitR1File.replace('R1','R2')
	tempFiles[prefix] = {}
	tempFiles[prefix]['R1'] = open(splitR1File,'w')
	tempFiles[prefix]['R2'] = open(splitR2File,'w')
	tempR1files.append(splitR1File)
	tempR2files.append(splitR2File)
    return tempFiles, tempR1files, tempR2files

def splitFiles(outputprefix, inFastq1, inFastq2, n, idxBase, constant):
    tempFiles, tempR1files, tempR2files = openTempFile(n, outputprefix)
    counter = 0
    with gzip.open(inFastq1,'rb') as fq1, gzip.open(inFastq2,'rb') as fq2:
	for read1,read2 in izip(FastqGeneralIterator(fq1),FastqGeneralIterator(fq2)):
	    counter += 1
	    idLeft, seqLeft, qualLeft = read1
	    idRight, seqRight, qualRight = read2
	    prefix = seqLeft[:n]
	    if seqLeft[idxBase:(idxBase + len(constant))] == constant and 'N' not in prefix:
		tempFiles[prefix]['R1'].write('@%s\n%s\n+\n%s\n' %(idLeft, seqLeft, qualLeft))
		tempFiles[prefix]['R2'].write('@%s\n%s\n+\n%s\n' %(idRight, seqRight, qualRight))
	    if counter % 1000000 == 0:
		stderr.write('Parsed %i records\n' %counter)
    [prefix[key].close() for prefix in tempFiles.values() for key in prefix]
    print 'closed all files'
    return tempR1files, tempR2files

def writeFile(read1, read2, leftReads, rightReads, outClusterCount):
    for left, right in zip(list(leftReads),list(rightReads)):
	assert left.split(' ')[0] == right.split(' ')[0], 'Wrong order pairs!!'
	outClusterCount += 1
	read1.write(left)
	read2.write(right)
    return outClusterCount

def main(outputprefix, inFastq1, inFastq2, idxBase, minReadCount, barcodeCutOff, threads, constant):
    """
    main function:
        controlling work flow
        1. generate read clusters by reading from fq1 and fq2
        2. obtain concensus sequence from read clusters
        3. writing concensus sequence to files
    """
    start = time.time()
    n_prefix = 3

    #print out parameters
    stderr.write( '[%s] Using parameters: \n' %(programname))
    stderr.write( '[%s]     indexed bases:                     %i\n' %(programname,idxBase))
    stderr.write( '[%s]     threads:                           %i\n' %(programname, threads))
    stderr.write( '[%s]     minimum coverage:                  %i\n' %(programname,minReadCount))
    stderr.write( '[%s]     outputPrefix:                      %s\n' %(programname,outputprefix))
    stderr.write( '[%s]     using constant regions:            %s\n' %(programname,constant))
    stderr.write( '[%s]     using N prefix to split:           %i\n' %(programname,n_prefix))

    
    # divide reads into subclusters
    tempR1files, tempR2files = splitFiles(outputprefix, inFastq1, inFastq2, n_prefix, idxBase, constant)
    read1File = outputprefix + '_R1_001.fastq.gz'
    read2File = outputprefix + '_R2_001.fastq.gz'
    outClusterCount = 0
    barcodeCount = np.array(0)
    barcode_family = 0

    #start writing to new file 
    with gzip.open(read1File,'wb') as read1, gzip.open(read2File,'wb') as read2:
	bar =  progressbar.ProgressBar(maxval=len(tempR1files),  
	    widgets=[progressbar.Bar('=', '[', ']'), 
		    ' ', progressbar.Percentage()])
	bar.start()
	status = 0
	for inFastq1, inFastq2 in zip(tempR1files, tempR2files):
	    if os.path.getsize(inFastq1) > 0:
		left, right, bcc, barcode_family_count = clustering(outputprefix, inFastq1, inFastq2, idxBase, 
				    minReadCount, barcodeCutOff, threads, constant)
		barcodeCount = np.append(barcodeCount,bcc)
		barcode_family += barcode_family_count
		if left != 0:
		    outClusterCount = writeFile(read1, read2, left, right, outClusterCount)
	    os.remove(inFastq1)
	    os.remove(inFastq2)
	    status += 1
	    bar.update(status)
    bar.finish()

    stderr.write('[%s] Extracted: %i barcodes sequence\n' %(programname,barcode_family))
    # ending processes, and plot barcode count
    p = plotBCdistribution(barcodeCount[barcodeCount > 0], outputprefix)        
    stderr.write('[%s] Finished writing error free reads\n' %programname)
    stderr.write('[%s]     read1:            %s\n' %(programname, read1File))
    stderr.write('[%s]     read2:            %s\n' %(programname, read2File))
    stderr.write('[%s]     output clusters:  %i\n' %(programname, outClusterCount))
    stderr.write('[%s]     time lapsed:      %2.3f min\n' %(programname, np.true_divide(time.time()-start,60)))
    return 0
        
if __name__ == '__main__':
    outputprefix, inFastq1, inFastq2, idxBase, minReadCount, barcodeCutOff, threads, constant = getOptions()
    main(outputprefix, inFastq1, inFastq2, idxBase, minReadCount, barcodeCutOff, threads, constant)
